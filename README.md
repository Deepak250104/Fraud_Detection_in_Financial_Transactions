# ğŸ’³ Fraud Detection in Financial Transactions

**Author:** Deepak250104

**Dataset:** [Google Drive Link](https://drive.usercontent.google.com/download?id=1VNpyNkGxHdskfdTNRSjjyNa5qC9u0JyV&export=download&authuser=0)

**Model:** LightGBM (Gradient Boosted Trees)

**Language:** Python 3.12.4

**Frameworks:** scikit-learn, LightGBM, pandas, matplotlib, seaborn

---

## ğŸ“˜ Project Overview

This project focuses on detecting fraudulent financial transactions using a machine learning approach. The dataset contains over 6.3 million records simulating anonymized transactions, with 10 engineered and raw features. The goal is to accurately classify transactions as fraudulent or legitimate using interpretable and scalable methods. The system is designed with modular notebooks and can be deployed in production environments.

---

## ğŸ“ Project Structure

```
Fraud_Detection_in_Financial_Transactions/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ Fraud.csv                  # Original dataset
â”‚   â”œâ”€â”€ processed.csv              # Preprocessed data
â”‚   â”œâ”€â”€ processed_features.csv     # Features after preprocessing
â”‚   â”œâ”€â”€ processed_target.csv       # Target labels
â”‚   â””â”€â”€ Data Dictionary.txt        # Feature descriptions
â”‚
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ lgbm_model.pkl             # Trained LightGBM model
â”‚   â””â”€â”€ preprocessor.pkl           # Data preprocessing pipeline
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ EDA.ipynb                 # Exploratory data analysis
â”‚   â”œâ”€â”€ feature_extraction.ipynb  # Feature engineering and transformations
â”‚   â”œâ”€â”€ model_selection.ipynb     # Model training and hyperparameter tuning
â”‚   â””â”€â”€ evaluation.ipynb          # Model performance and final validation
â”‚
â”œâ”€â”€ LICENSE
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt              # All dependencies
```

---

## ğŸ”¢ Dataset Summary

* **Size:** \~6.3 million rows
* **Features:** 10 columns including `amount`, `oldbalanceOrg`, `newbalanceOrig`, `type`, etc.
* **Target Variable:** `isFraud` (0 = normal, 1 = fraudulent)
* **Class Imbalance:**

  * **Fraudulent transactions:** \~0.129% of total data
  * This extreme imbalance was addressed using:

    * `class_weight="balanced"` in LightGBM
    * Stratified sampling during cross-validation
    * Emphasis on precision, recall, and AUC instead of accuracy

---

## ğŸ§¹ Data Cleaning & Preprocessing

* **Missing Values:** None in the dataset, verified in EDA.
* **Outliers:** Tree-based models handle them well; no removal needed.
* **Multicollinearity:** Not problematic for LightGBM; no explicit removal.
* **Dropped Columns:**

  * `nameOrig`, `nameDest` (identifiers)
  * `isFlaggedFraud` (data leakage)
* **Engineered Features:**

  * `errorBalanceOrig = oldbalanceOrg - newbalanceOrig - amount`
  * `errorBalanceDest = newbalanceDest - oldbalanceDest - amount`
    These features capture inconsistencies in the transaction flow and significantly improve predictive power.

---

## ğŸ§  Model Description

We used **LightGBM**, a gradient boosting algorithm optimized for performance and scale. Key reasons for selection:

* Handles large-scale data efficiently
* Robust to skewed distributions and multicollinearity
* Supports automatic handling of categorical features
* Fast training and inference

### ğŸ” Hyperparameter Optimization

Used `RandomizedSearchCV` on parameters:

* `num_leaves`
* `learning_rate`
* `n_estimators`
* `max_depth`
* `min_child_samples`

Cross-validation ensured that tuning was not overfit to a single train-test split.

---

## ğŸ“Š Model Performance

Achieved **outstanding results** on the holdout set:

| Metric    | Value  |
| --------- | ------ |
| Accuracy  | 100.0% |
| Precision | 100.0% |
| Recall    | 100.0% |
| F1-Score  | 1.000  |
| AUC-ROC   | 0.9998 |

* **Confusion Matrix**: No false positives or false negatives
* **ROC Curve**: AUC close to 1, indicating near-perfect separation between fraud and non-fraud

> âš ï¸ Caution: These metrics may indicate possible data leakage or overfitting. All pipeline steps, especially preprocessing and feature engineering, were verified to avoid this.

---

## ğŸ” Key Predictive Factors

* **Transaction Type**: `TRANSFER` and `CASH_OUT` were frequently associated with fraud
* **Amount**: Extremely high or low transaction values often flagged as suspicious
* **Balance Mismatch Features**: Engineered error features revealed fraudulent behavior when account balances didnâ€™t reflect legitimate transaction logic

These factors align with common fraud patterns, such as draining balances, mimicking real user behavior, or inconsistent balance states.

---

## ğŸ›¡ï¸ Infrastructure Recommendations

To prevent fraud effectively, financial institutions should:

* **Deploy real-time fraud detection models** like this one
* **Log model decisions** for auditability and compliance
* **Update models periodically** to adapt to evolving fraud tactics
* **Use human-in-the-loop systems** for high-stake decisions
* **Ensure secure feature engineering** to prevent leakage from future data or labels

---

## ğŸ“ˆ Post-deployment Monitoring

Once integrated, success should be measured by:

* Reduction in fraud losses
* Monitoring false positives (blocked genuine users)
* Real-world precision/recall vs validation scores
* Feedback from manual investigation teams
* A/B testing against baseline systems

---

## ğŸ“¦ Installation

Create a virtual environment and install dependencies:

```bash
pip install -r requirements.txt
```

Run notebooks in order:

1. `EDA.ipynb`
2. `feature_extraction.ipynb`
3. `model_selection.ipynb`
4. `evaluation.ipynb`

---

## ğŸ” License

This project is licensed under the MIT License.

## Author 

[Deepak250104](https://github.com/Deepak250104)
